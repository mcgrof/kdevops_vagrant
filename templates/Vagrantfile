# -*- mode: ruby -*-
# # vi: set ft=ruby :

# This file should only be edited to *grow* support for new features.
# If you are not interested in that and are happy with what is provided,
# all you'd have to do is edit your respective project name space
# yaml file to set the hosts / playbooks you need to run.
#
# We infer your project namespace from the parent directory where this
# vagrant directory sits in. Suppose this file is in your project under
# kdevops/vagrant/Vagrantfile then your namespace is inferred to be kdevops.
#
# The file which describes nodes for this project is expected to then be
# kdevops_nodes.yml. In this project example's case you can override node
# configuration through the node overridef file:
#
# 	kdevops_vagrant_nodes_override.yaml
#
# You can use environemnt variables as well to override configuration, but this
# doesn't scale well as it means we have to add a support into this file for
# each new feature.
#
# Environment you can use are expected to be prefixed in upper case by your
# project namespace. We support the following environment variables, assume
# ${PN_U} below is your upper case of your project namespace, so for kdevops
# this would be KDEVOPS:
#
# ${PN_U}_VAGRANT_NODE_CONFIG - lets you override default configuration
#   file used
# ${PN_U}_VAGRANT_PROVIDER - lets you override provider
# ${PN_U}_VAGRANT_LIMIT_BOXES - lets you enable box limiting
# ${PN_U}_VAGRANT_LIMIT_NUM_BOXES - number of boxes to limit

Vagrant.require_version ">= 1.6.0"

require 'json'
require 'yaml'
require 'fileutils'
require 'rbconfig'

@os = RbConfig::CONFIG['host_os']

# XXX: upstream libvirt enhancement needed
#
# Vagrant allows multiple provider code to be supported easily, however
# this logic assuems all provider setup is supported through configuration
# variables from the provider. This is not the case for libvirt. We need
# to create the qemu image, and the libvirt vagrant provider doesn't have
# support to create the images as virtual box does. We run the commands
# natively, however this also reveals that on execution path even code
# for other providers gets executed regardless of the provider you are
# using *unless* that code is using provider specific variables. Best we
# can do for now is detect your OS and set a local variable where we *do*
# run different local code paths depending on the target provider.
#
# Right now we make these assumptions:
#
# Your OS            Provider
# -------------------------------
# Linux              libirt
# Mac OS X           virtualbox
provider = "libvirt"

case
when @os.downcase.include?('linux')
  provider = "libvirt"
when @os.downcase.include?('darwin')
  provider = "virtualbox"
else
  puts "You OS hasn't been tested yet, go add support and send a patch."
  exit
end

# Default ansible inventory file
inventory_default = File.dirname(__FILE__) + "/../hosts"

# We assume if you are in kdevops/vagrant/ your project namespace is kdevops
#                         oscheck/vagrant/ your project namespace is oscheck
project_namespace = File.basename(File.dirname(Dir.getwd))

# shorthand accessors
pn_l = project_namespace.downcase
pn_u = project_namespace.upcase

# Bash variables cannot use "-", they must use _
pn_u = pn_u.tr("-", "_")
pn_l = pn_l.tr("-", "_")

node_config          = ENV[pn_u + '_VAGRANT_NODE_CONFIG'] ? ENV[pn_u + '_VAGRANT_NODE_CONFIG'] : pn_l + "_nodes.yaml"
node_config_override = ENV[pn_u + '_VAGRANT_NODE_CONFIG'] ? ENV[pn_u + '_VAGRANT_NODE_CONFIG'] : pn_l + "_nodes_override.yaml"

config_data = YAML.load_file(node_config)

override_data = false
if File.file?(node_config_override)
  override_data = true
  config_data_override = YAML.load_file(node_config_override)
end

global_data = config_data['vagrant_global']
vagrant_boxes = config_data['vagrant_boxes']
ansible_playbooks = config_data['ansible_playbooks'] ? config_data['ansible_playbooks'] : false

if override_data
  if config_data_override['vagrant_global']
    global_data = config_data_override['vagrant_global']
  end
  if config_data_override['vagrant_boxes']
    vagrant_boxes = config_data_override['vagrant_boxes']
  end
  if config_data_override['ansible_playbooks']
    ansible_playbooks = config_data_override['ansible_playbooks'] ? config_data_override['ansible_playbooks'] : false
  end
end

if global_data['force_provider']
  provider = global_data['force_provider']
end

if ENV[pn_u + '_VAGRANT_PROVIDER']
  provider = ENV[pn_u + '_VAGRANT_PROVIDER']
end

supported_provider = case provider
  when "virtualbox" then true
  when "libvirt" then true
  else false
end

if ! supported_provider
  puts "Unsupported provider: #{provider} on " + RbConfig::CONFIG['host_os']
  puts "Consider adding support and send a patch"
  exit
end

limit_boxes = global_data['limit_boxes'] == "yes" ? "yes" : "no"
limit_boxes = ENV[pn_u + '_VAGRANT_LIMIT_BOXES'] == "yes" ? "yes" : limit_boxes

limit_num_boxes = vagrant_boxes.count
limit_max_num_boxes = limit_num_boxes
limit_num_boxes = limit_boxes == "yes" && global_data['limit_num_boxes']  ? [limit_max_num_boxes, global_data['limit_num_boxes']].min : limit_num_boxes
limit_num_boxes = limit_boxes == "yes" && ENV[pn_u + '_VAGRANT_LIMIT_NUM_BOXES'] ? [limit_max_num_boxes, ENV[pn_u + '_VAGRANT_LIMIT_NUM_BOXES'].to_i].min : limit_num_boxes

qemu_group = global_data['libvirt_cfg']['qemu_group']
qemu_group_auto = global_data['libvirt_cfg']['qemu_group_auto']

if qemu_group_auto
  if File.exists?('/etc/debian_version')
    qemu_group = 'libvirt-qemu'
  elsif File.exists?('/etc/redhat-release') or File.exists?('/etc/centos-release')
    qemu_group = 'qemu'
  elsif File.exists?('/etc/SuSE-release') or File.exists?('/etc/SUSE-brand')
    qemu_group = 'qemu'
  end
end

if ENV[pn_u + '_VAGRANT_QEMU_GROUP']
  qemu_group = ENV[pn_u + '_VAGRANT_QEMU_GROUP']
end

num_servers = limit_num_boxes.to_i
playbooks_to_run = ansible_playbooks && ansible_playbooks['playbooks'] ? ansible_playbooks['playbooks'].size() : 0
ansible_ran = false

Vagrant.configure("2") do |config|
  box_count = 0
  vagrant_boxes.each do |server_data|
    if limit_boxes == "yes" && box_count >= num_servers
      box_count+=1
      next
    end
    # Using sync folders won't work for say openstack / aws / azure / gce, and
    # for that we'll use terraform, so best to allow whatever data we want to
    # sync be provisioned with ansible.
    config.vm.synced_folder './', '/vagrant', type: '9p', disabled: true, accessmode: "mapped", mount: false
    config.vm.define server_data["name"] do |srv|
      srv.vm.box = server_data['box'] ? server_data['box'] : global_data["box"]
      srv.vm.box_version = server_data['box_version'] ? server_data['box_version'] : global_data["box_version"]
      srv.vm.network "private_network", ip: server_data["ip"]
      host_name = server_data["name"]
      nvme_path = File.dirname(__FILE__) + "/" + global_data['nvme_disk_path'] + "/#{host_name}"
      srv.vm.provider "virtualbox" do |vb, override|
	if provider == "virtualbox" && ! global_data['virtualbox_cfg']['auto_update']
          # we'll need to run later: vagrant vbguest install
          config.vbguest.auto_update = false
        end
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        vb.memory = global_data["memory"]
        vb.cpus = global_data["cpus"]
        if global_data['nvme_disks']
          port = 0
          port_count = global_data['nvme_disks'].size()
          FileUtils.mkdir_p nvme_path
          global_data['nvme_disks'].each do |key, value|
            size = value['size']
	    # XXX: the purpose of the nvme drive is unused for now but later we
	    # can let ansible provision it for our target mount paths
            purpose = key
            port_plus = port + 1
            nvme_disk_postfix = global_data['virtualbox_cfg']['nvme_disk_postfix']
            nvme_disk = nvme_path + "/nvme#{port}n#{port_plus}.#{nvme_disk_postfix}"

            # "Standard" provides a sparse file. That's what we want, we cheat
            # the OS and only use what we need. If you want the real file size
            # add a global config option and send a patch and justify it. I'd
            # like to hear about it. We use sparse files for libvirt as well
            # and should try to keep setup in sync.
            if (! File.file?(nvme_disk))
              vb.customize ["createmedium", "disk", "--filename", nvme_disk, "--variant", "Standard", "--format", nvme_disk_postfix.upcase, "--size", size]
            end
            # Virtualbox supports only one nvme controller... this will fail
            # unless you are a Virtualbox hacker adding support for this :)
            if global_data['virtualbox_cfg']['nvme_controller_per_disk']
              # https://www.virtualbox.org/manual/ch08.html#vboxmanage-storagectl
              # This command attaches, modifies, and removes a storage
              # controller. After this, virtual media can be attached to the
              # controller with the storageattach command.
              nvme_name = "nvme#{port}"
              if (! File.file?(nvme_disk))
                vb.customize ["storagectl", :id, "--name", "#{nvme_name}", "--add", "pcie", "--controller", "NVMe", "--portcount", 1, "--bootable", "off"]
              end
	      # Now attach the drive
	      vb.customize ["storageattach", :id, "--storagectl", "#{nvme_name}", "--type", "hdd", "--medium", nvme_disk, "--port", 0]
            else
              if (port == 0 && !File.file?(nvme_disk))
                vb.customize ["storagectl", :id, "--name", "nvme0", "--add", "pcie", "--controller", "NVMe", "--portcount", port + 1, "--bootable", "off"]
              end
	      vb.customize ["storageattach", :id, "--storagectl", "nvme0", "--type", "hdd", "--medium", nvme_disk, "--port", port]
            end

            if global_data['enable_sse4']
	      # Support for the SSE4.x instruction is required in some versions of VB.
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.1", "1"]
              vb.customize ["setextradata", :id, "VBoxInternal/CPUM/SSE4.2", "1"]
            end
            port += 1
          end # end of looping all nvme disks
        end # end of checking for nvme disks
      end # end of virtualbox provider section
      # For details see: https://github.com/vagrant-libvirt/vagrant-libvirt
      srv.vm.provider "libvirt" do |libvirt, override|
        #libvirt.host = "localhost"
        override.vm.hostname = host_name
        override.vm.boot_timeout = global_data['boot_timeout']
        libvirt.watchdog :model => 'i6300esb', :action => 'reset'
	libvirt.storage_pool_path = File.dirname(__FILE__)
        libvirt.memory = global_data["memory"]
        libvirt.cpus = global_data["cpus"]
        libvirt.emulator_path = global_data['libvirt_cfg']['emulator_path']
        if server_data["machine_type"]
          libvirt.machine_type = server_data["machine_type"]
        else
          if global_data['libvirt_cfg']['machine_type']
            libvirt.machine_type = global_data['libvirt_cfg']['machine_type']
          end
        end
        if global_data['nvme_disks']
          port = 0
          port_count = global_data['nvme_disks'].size()
          FileUtils.mkdir_p nvme_path
          global_data['nvme_disks'].each do |key, value|
            size = value['size']
            purpose = key
            port_plus = port + 1
	    key_id_prefix = global_data['libvirt_cfg']['nvme_disk_id_prefix']
	    disk_id = "#{key_id_prefix}#{port}"
	    serial_id = "serial#{port}"
            nvme_disk_postfix = global_data['libvirt_cfg']['nvme_disk_postfix']
            nvme_disk = nvme_path + "/nvme#{port}n#{port_plus}.#{nvme_disk_postfix}"
            unless File.exist? (nvme_disk)
	      if provider == "libvirt"
	        cmd = "qemu-img create -f qcow2 #{nvme_disk} #{size}M"
	        ok = system(cmd)
	        if ! ok
                  puts "Command failed: #{cmd}"
                  exit
                end
              end
            end
            non_nvme_drive_interface = "virtio"
            if server_data['lacks_drive_virtio']
              non_nvme_drive_interface = "ide"
            end
            if server_data['lacks_nvme_support']
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{nvme_disk},if=#{non_nvme_drive_interface}"
            else
              libvirt.qemuargs :value => "-drive"
              libvirt.qemuargs :value => "file=#{nvme_disk},if=none,id=#{disk_id}"
              libvirt.qemuargs :value => "-device"
              libvirt.qemuargs :value => "nvme,drive=#{disk_id},serial=#{serial_id}"
            end
            port += 1
	  end
	  if provider == "libvirt"
	    cmd = "chgrp -R #{qemu_group} #{nvme_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	    cmd = "chmod -R g+rw #{nvme_path}"
	    ok = system("#{cmd}")
	    if ! ok
              puts "Command failed: #{cmd}"
              exit
            end
	  end # end of provider check for libvirt
	end # end of check for nvme disks for libvirt
      end # end of libvirt provider code
      playbooks_to_run = ansible_playbooks && ansible_playbooks['playbooks'] ? ansible_playbooks['playbooks'].size() : 0
      playbook_num = 0
      run_ansible = server_data['skip_ansible'] ? false : true
      if run_ansible && !ansible_ran && box_count + 1 >= num_servers && playbooks_to_run
        playbooks = ansible_playbooks['playbooks']
        extra_vars = ansible_playbooks['extra_vars'] ? File.dirname(__FILE__) + "/" + ansible_playbooks['extra_vars'] : ""
        inventory = ansible_playbooks['inventory'] ? File.dirname(__FILE__) + "/" + ansible_playbooks['inventory'] : inventory_default
        playbooks.each do |playbook|
          srv.vm.provision "my_playbook_#{playbook_num}", type:'ansible' do |ansible|
	    playbook_num = playbook_num + 1
            ansible.limit = playbook['limit'] ? playbook['limit'] : "all"
            ansible.playbook = playbook['name']
            if inventory != "" && File.exist?(inventory)
              ansible.raw_arguments = ["--inventory", "#{inventory}" ]
            end # end of inventory and file check for inventory
            if extra_vars != "" && File.exist?(extra_vars)
              ansible.extra_vars = "@#{extra_vars}"
            end # end of check if extra vars is set and if file exists
	  end # end of ansible
        end # end of playbooks
	playbook_num = playbook_num + 1
	if playbook_num >= playbooks_to_run
          ansible_ran = true
        end
      end # end of check for the last server and ansible
    end # end of srv defined loop
    box_count+=1
  end # end of vagrant_boxes loop
end
